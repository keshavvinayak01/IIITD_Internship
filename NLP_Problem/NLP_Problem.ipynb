{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as f\n",
    "import spacy\n",
    "import sklearn as sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I tried using logistic regression, MultiPerceptron, and tried to use CNNs and LSTMs for this task, couldn't fit the last two because of time issues and the fact that i couldn't download the gLOVe vectors perfectly due to network issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/keshavpc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"datasets/V1.4_Training.csv\")#loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.concatenate((['\"Please enable removing language code from the Dev Center \"language history\" For example if you ever selected \"ru\" and \"ru-ru\" laguages and you published this xap to the Store then it causes Tile localization to show the en-us(default) tile localization which is bad.\"'],\n",
    "                        train.iloc[:]['\"Please enable removing language code from the Dev Center \"language history\" For example if you ever selected \"ru\" and \"ru-ru\" laguages and you published this xap to the Store then it causes Tile localization to show the en-us(default) tile localization which is bad.\"'])\n",
    "                       ,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following function corrects the common mishappenings of a sentence which may hinder the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove whitespaces\n",
    "def preprocess(texts):\n",
    "    \n",
    "    texts = [text.replace('\"','') for text in texts]\n",
    "    texts = [text.lower().strip() for text in texts]\n",
    "\n",
    "\n",
    "    # Remove numbers\n",
    "    texts = [re.sub(r'\\d+', '', text) for text in texts]\n",
    "\n",
    "\n",
    "    #Remove Punctuation\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for i in range(len(texts)):\n",
    "        texts[i] = ' '.join(tokenizer.tokenize(texts[i]))\n",
    "        \n",
    "    # REMOVE STOPWORDS\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    texts = [' '.join([word for word in text.split() if word not in STOPWORDS]) for text in texts ] \n",
    "    \n",
    "    return texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = preprocess(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate(([1],train.iloc[:]['1'].values),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = pd.DataFrame({'texts':[],'labels':[],'len':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe['texts'] = texts\n",
    "new_dataframe['labels'] = labels\n",
    "# Now bucketing the data, as the texts are of different lengths.\n",
    "new_dataframe['len'] = [len(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8500"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_dataframe)):\n",
    "    if(new_dataframe.loc[i].len == 0):\n",
    "        new_dataframe = new_dataframe.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8478"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above steps removed null sentences(22) from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = new_dataframe['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = new_dataframe['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"datasets/test_labelled.csv\",encoding = \"ISO-8859-1\", engine='python')#loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess(test['sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess(test['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_a = new_dataframe[new_dataframe.labels == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_b = new_dataframe[new_dataframe.labels == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_a = [class_a,class_a,class_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_a_final = pd.concat(class_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6255"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_a_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "merging = [class_a_final,class_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = pd.concat(merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(new_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data\n",
    "from sklearn.utils import shuffle\n",
    "new_dataframe = shuffle(new_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above steps basically dealing with the class imbalance problem in the dataset, as the labels had the ratio of 3:1, upscales the class with less samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_trains = [new_dataframe[:length//3],new_dataframe[length//3:2*length//3],new_dataframe[2*length//3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in three_trains:\n",
    "    X_train.append(i['texts'].values)\n",
    "    y_train.append(i['labels'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1,10),token_pattern='(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l2', C=1.0,solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing logistic regression over the three segmented datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train[i]).toarray()\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(list(X_test)).toarray()\n",
    "    model.fit(X_train_tfidf, y_train[i])\n",
    "    predictions.append(model.predict(X_test_tfidf))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training over the three seperately to get an average of the performance in above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_predictions in predictions:\n",
    "    accuracies.append(sum(y_test == y_predictions)*100/len(y_test))\n",
    "    f1_scores.append(f1_score(y_test, y_predictions, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70.27027027027027, 70.4391891891892, 71.45270270270271]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7026891293430124, 0.7037757158991809, 0.7138403365911281]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ~ 70% accuracy using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='adam', alpha=1e-5,learning_rate_init=3e-2,learning_rate='adaptive',random_state=40,\n",
    "                   early_stopping=True,hidden_layer_sizes=(200,32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Took around 10 minutes\n",
    "predictions_mlp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train[i]).toarray()\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(list(X_test)).toarray()\n",
    "    clf.fit(X_train_tfidf, y_train[i])\n",
    "    predictions_mlp.append(clf.predict(X_test_tfidf))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Tried using LSTMs and CNN models for this task however i failed to catch errors which prevented it from showing better scores in the given time frame.\n",
    "#### The bad performance is most probably due to the loose tokenization by tf-idf, i wanted to use predefined gLOve ones but I couldn't download them due to internet issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_mlp = []\n",
    "f1_scores_mlp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_predictions in predictions_mlp:\n",
    "    accuracies_mlp.append(sum(y_test == y_predictions)*100/len(y_test))\n",
    "    f1_scores_mlp.append(f1_score(y_test, y_predictions, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60.979729729729726, 63.3445945945946, 64.86486486486487]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6388872903886658, 0.6545306629333962, 0.6402779286274118]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained using the default perceptron by sklearn, not a significent boost in performance, indicating that the tf-idf vectorization was not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now importing the actual test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasets/SubtaskA_EvaluationData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This would enable live traffic aware apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = np.concatenate((['This would enable live traffic aware apps.'],\n",
    "                        test.iloc[:]['This would enable live traffic aware apps.'])\n",
    "                       ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(list(preprocess(X_test_final))).toarray()\n",
    "y_predicted = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataframe = pd.DataFrame({'texts':X_test_final,'labels':y_predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataframe.to_csv('keshav_vinayak_jha.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
